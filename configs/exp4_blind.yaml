random_seed: 0b011011
run_name: "temp"
# blind

#-----------------------------------------------
#1. Dataset
#-----------------------------------------------

dataset:
  ratio: 0.001
  # ratio: 1
  enhance: True

  wb_train: [
              "/home/woongjib/Projects/Dataset_BESSL/Dataset_gt_crop/FSD50K_CORE_M4a",  
              "/home/woongjib/Projects/Dataset_BESSL/Dataset_gt_crop/MUSDB_CORE_M4a", 
              "/home/woongjib/Projects/Dataset_BESSL/Dataset_gt_crop/VCTK_CORE_M4a"
              ]
  nb_train: [
              "/home/woongjib/Projects/Dataset_BESSL/Dataset_core_crop/FSD50K_CORE_M4a",  
              "/home/woongjib/Projects/Dataset_BESSL/Dataset_core_crop/MUSDB_CORE_M4a", 
              "/home/woongjib/Projects/Dataset_BESSL/Dataset_core_crop/VCTK_CORE_M4a"
              ]  
  wb_test: [
              "/home/woongjib/Projects/USAC44_mono_48k", 
              ]
  nb_test: [
              "/home/woongjib/Projects/USAC44_mono_48k_HEAAC16_LPF_Crop", 
              ]

  batch_size: 8
  seg_len: 0.9
  num_workers: 8
 
#-----------------------------------------------
#2. Model
#-----------------------------------------------

generator:
  type: SEANet
  # hubert, w2v, wavlm
  kmeans_path: None
  in_channels: 32
  fe_weight_path: None
  min_dim: 16
  train_enc: True

discriminator: 
  type: MultiBandSTFTDiscriminator
  MultiBandSTFTDiscriminator_config:
      C: 32
      n_fft_list: [2048, 1024, 512]
      hop_len_list: [512, 256, 128]
      band_split_ratio:
          - [0.0, 0.1] # check only 4.5 kHz above
          - [0.1, 0.25] # 0.1 -> 0.1875
          - [0.25, 0.5]
          - [0.5, 0.75]
          - [0.75, 1.0]

#-----------------------------------------------
#3. Loss
#-----------------------------------------------
loss:
  ms_mel_loss_config:
            n_fft_list: [32, 64, 128, 256, 512, 1024, 2048]
            hop_ratio: 0.25
            mel_bin_list: [5, 10, 20, 40, 80, 160, 320]
            reduction: mean
            loss_ratio: 1.0
            sr: 48000
            fmin: 0

  lambda_mel_loss: 15
  lambda_fm_loss: 2
  lambda_adv_loss: 1
  lambda_commitment_loss: 0 # only for RVQ models
  lambda_codebook_loss: 0


#-----------------------------------------------
#4. Optimizer (ADAM)
#-----------------------------------------------

optim:
  learning_rate: 0.001
  scheduler_gamma: 1
  B1: 0.5
  B2: 0.9


#-----------------------------------------------
#Training
#-----------------------------------------------
train:
  # epoch_save_start: 1
  val_epoch: 1
  
  ckpt_save_dir: "./ckpts/temp2"
  max_epochs: 20

  devices:
    - 0
    #- 1
    # -2 ... if you are using DDP

  # True if load from previous
  ckpt: False
  ckpt_path: ""